{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84f8ec7",
   "metadata": {},
   "source": [
    "# Air Quality Prediction\n",
    "\n",
    "[World's Air Pollution: Real-Time Air Quality Index](https://waqi.info/)\n",
    "\n",
    "https://aqicn.org/json-api/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ae37b",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0966c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Make random numbers stable for reproduction\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Add sources to the path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = str(Path().resolve().parent)\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from src.data import aqi, meteo\n",
    "from src.data.calendar import add_calendar_features\n",
    "from src.data.features import FeatureScaler, split_to_windows, flatten_windows, _flatten_windows # TODO: don't use internal methods\n",
    "from src.model.training import split_data\n",
    "from src.model.evaluation import evaluate_iaqi_predictions, create_metrics_dataframe, get_day_n_metrics\n",
    "from src.model.inference import recursive_forecasting\n",
    "from src.model import xgboost\n",
    "from src.hopsworks.client import HopsworksClient\n",
    "from src.common import LOGGER_NAME, IAQI_FEATURES\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\n",
    ")\n",
    "\n",
    "LOGGER = logging.getLogger(LOGGER_NAME)\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b110432",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many (lagged) days to use as input during training\n",
    "historical_window_size = 3\n",
    "# How many days to teach the model to predict \n",
    "prediction_window_size = 3\n",
    "# TODO: use recursive_forecasting boolean instead\n",
    "# How many predictions to do as part of recursive forecasting\n",
    "num_of_predictions = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dc8f9",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a4f37",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df = aqi.load_data(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ac840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure column names are stripped\n",
    "aqi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b49f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24032e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb98a5",
   "metadata": {},
   "source": [
    "### Handle missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df = aqi.clean_missing_dates(aqi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7144b2",
   "metadata": {},
   "source": [
    "### Handle N/A values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85accb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df = aqi.clean_missing_values(aqi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9f8bc",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51426c85",
   "metadata": {},
   "source": [
    "### Calendar Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3305f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df = add_calendar_features(aqi_df)\n",
    "aqi_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bc8e8",
   "metadata": {},
   "source": [
    "### Cyclical features\n",
    "\n",
    "TODO: \n",
    "For cyclical features (month, day_of_week, hour, day_of_year), it's often better to transform them\n",
    "into sine and cosine components to preserve the cyclical nature and avoid arbitrary ordinal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1572b",
   "metadata": {},
   "source": [
    "### Meteorological data\n",
    "\n",
    "[Meteorological Parameters](https://dev.meteostat.net/formats.html#meteorological-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5308d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = meteo.fetch_daily_data(aqi_df)\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3341809",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = meteo.clean_missing_values(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba59af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge_asof(aqi_df, weather_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5883bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52e600",
   "metadata": {},
   "source": [
    "### Target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6097e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = all_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382bc29",
   "metadata": {},
   "source": [
    "### Convert all features to float\n",
    "\n",
    "Converting all features to floats is a fundamental preprocessing step for neural networks. It ensures compatibility with the underlying mathematical operations, facilitates normalization, and aligns with the requirements of deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2023169",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a124568",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44f834",
   "metadata": {},
   "source": [
    "## Explore trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for year, group in aqi_df.groupby(\"year\"):\n",
    "    if (year in [2022, 2023]):\n",
    "        plt.plot(group[\"day_of_year\"], group[\"pm25\"], label=str(year), alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Day of Year\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.title(\"Yearly PM2.5 Trends Overlaid by Day of Year\")\n",
    "plt.legend(title=\"Year\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(aqi_df[\"pm25\"], lags=365)\n",
    "plt.title(\"Autocorrelation Function (ACF) for PM2.5\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x=\"month\", y=\"pm25\", data=aqi_df)\n",
    "plt.title(\"Seasonal Subseries Plot: PM2.5 by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ee504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure pm25 is float and has no missing values for decomposition\n",
    "pm25_series = aqi_df[\"pm25\"].astype(float).interpolate()\n",
    "\n",
    "result = seasonal_decompose(pm25_series, model='additive', period=365)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "result.plot()\n",
    "plt.suptitle(\"Seasonal Decomposition of PM2.5 Time Series\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in pm25 for spectral analysis\n",
    "pm25_filled = aqi_df[\"pm25\"].astype(float).values\n",
    "\n",
    "# Compute the periodogram\n",
    "freqs, power = periodogram(pm25_filled)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(freqs, power)\n",
    "plt.title(\"Spectral Analysis (Periodogram) of PM2.5\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Power Spectral Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30711dd",
   "metadata": {},
   "source": [
    "## Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be938055",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_data(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5bc58",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Not strictly required for tree-based models (Random Forest, Gradient Boosting like XGBoost/LightGBM) as they are scale-invariant, but also don't hurt performance of these models.\n",
    "However, it is a must for Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ebad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = FeatureScaler()\n",
    "feature_scaler.fit(train_df)\n",
    "\n",
    "train_df = feature_scaler.transform(train_df)\n",
    "val_df = feature_scaler.transform(val_df)\n",
    "test_df = feature_scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b53a6f",
   "metadata": {},
   "source": [
    "## Prepare prediction windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_window_train,\n",
    "    X_window_val,\n",
    "    X_window_test,\n",
    "    y_window_train,\n",
    "    y_window_val,\n",
    "    y_window_test,\n",
    ") = split_to_windows(\n",
    "    train_df, val_df, test_df, historical_window_size, prediction_window_size, target_columns=target_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd519ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_window_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_window_train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fe0ad",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten for regressors\n",
    "X_flat_train, X_flat_val, X_flat_test, y_flat_train, y_flat_val, y_flat_test = flatten_windows(X_window_train, X_window_val, X_window_test, y_window_train, y_window_val, y_window_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flat_train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad147c56",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ed631",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = xgboost.create_regressor()\n",
    "xgboost_model.fit(X_flat_train, y_flat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predictions = recursive_forecasting(xgboost_model, test_df, historical_window_size, prediction_window_size, num_of_predictions, torch=False)\n",
    "\n",
    "predictions = [feature_scaler.inverse_transform(prediction) for prediction in predictions]\n",
    "actual = [feature_scaler.inverse_transform(value) for value in actual]\n",
    "\n",
    "xgboost_prediction_metrics = evaluate_iaqi_predictions(actual, predictions, prediction_window_size, num_of_predictions)\n",
    "xgboost_prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xboost_metrics_df = create_metrics_dataframe(xgboost_prediction_metrics)\n",
    "print(xboost_metrics_df.to_string(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73780988",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "for i in reversed(range(historical_window_size)):\n",
    "    for column in merged_df.columns:\n",
    "        feature_names.append(f\"{column}_lag_{i + 1}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from each target model\n",
    "importance_df = pd.DataFrame()\n",
    "\n",
    "for i, target_name in enumerate(merged_df.columns):  # Replace with your target names\n",
    "    estimator = xgboost_model.estimators_[i]\n",
    "    importance_df[target_name] = estimator.feature_importances_\n",
    "\n",
    "# Set feature names as index\n",
    "importance_df.index = feature_names  # Set index to feature names\n",
    "\n",
    "# Display top features for each target\n",
    "for target in importance_df.columns:\n",
    "    print(f\"\\n{target} - Top 10 Features:\")\n",
    "    target_imp = importance_df[target].sort_values(ascending=False).head(10)\n",
    "    for i, (feat, score) in enumerate(target_imp.items()):\n",
    "        print(f\"{i+1:2d}. {feat:<20} : {score:.4f}\")\n",
    "\n",
    "# Overall top features (averaged across all targets)\n",
    "overall_top = importance_df.mean(axis=1).sort_values(ascending=False).head(10)\n",
    "print(f\"\\nOverall Top 10 Features:\")\n",
    "for i, (feat, score) in enumerate(overall_top.items()):\n",
    "    print(f\"{i+1:2d}. {feat:<20} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3201ac0",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to monitor validation loss and save best weights\"\"\"\n",
    "\n",
    "    def __init__(self, patience=10, min_delta=1e-6, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            # Validation loss improved\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            # Save best weights\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = {\n",
    "                    k: v.clone().detach() for k, v in model.state_dict().items()\n",
    "                }\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "    def restore_best_weights_to_model(self, model):\n",
    "        \"\"\"Restore the best weights to the model\"\"\"\n",
    "        if self.best_weights is not None:\n",
    "            model.load_state_dict(self.best_weights)\n",
    "            print(f\"Restored best weights (val_loss: {self.best_loss:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12596b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM: convert windowed data to tensors\n",
    "def windows_to_tensor(X_windows, y_windows):\n",
    "    # X: list of DataFrames, each (window_size, num_features)\n",
    "    # y: list of DataFrames, each (prediction_window_size, num_targets)\n",
    "    X_tensor = torch.tensor(\n",
    "        np.stack([x.values for x in X_windows]), dtype=torch.float32\n",
    "    )\n",
    "    y_tensor = torch.tensor(\n",
    "        np.stack([y.values for y in y_windows]), dtype=torch.float32\n",
    "    )\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_train, y_lstm_train = windows_to_tensor(X_window_train, y_window_train)\n",
    "X_lstm_val, y_lstm_val = windows_to_tensor(X_window_val, y_window_val)\n",
    "X_lstm_test, y_lstm_test = windows_to_tensor(X_window_test, y_window_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd915e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lstm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f53258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim, num_layers=1, prediction_window_size=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim * prediction_window_size)\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.prediction_window_size = prediction_window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        out, _ = self.lstm(x)\n",
    "        # Use last hidden state for prediction\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.fc(out)\n",
    "        # Reshape to (batch, prediction_window_size, output_dim)\n",
    "        out = out.view(-1, self.prediction_window_size, self.output_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_lstm_train.shape[2]\n",
    "output_dim = y_lstm_train.shape[2]\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "prediction_window_size = y_lstm_train.shape[1]\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    input_dim, hidden_dim, output_dim, num_layers, prediction_window_size\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=1e-6)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_lstm_train.size(0))\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_lstm_train.size(0), batch_size):\n",
    "        idx = permutation[i : i + batch_size]\n",
    "        batch_X, batch_y = X_lstm_train[idx], y_lstm_train[idx]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_X.size(0)\n",
    "    epoch_loss /= X_lstm_train.size(0)\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_lstm_val)\n",
    "        val_loss = criterion(val_output, y_lstm_val).item()\n",
    "    \n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {epoch_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Best Val: {early_stopping.best_loss:.6f}\"\n",
    "    )\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        best_epoch = epoch + 1 - early_stopping.patience\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "        print(f\"Best epoch was {best_epoch} with val_loss: {early_stopping.best_loss:.6f}\")\n",
    "        break\n",
    "\n",
    "    # Update best epoch if this is the best so far\n",
    "    if val_loss == early_stopping.best_loss:\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "# Restore best weights\n",
    "early_stopping.restore_best_weights_to_model(model)\n",
    "\n",
    "# Training completed\n",
    "final_status = \"Early stopped\" if early_stopping.early_stop else \"Completed\"\n",
    "print(f\"\\nTraining {final_status.lower()} after {epoch + 1} epochs\")\n",
    "print(f\"Best validation loss: {early_stopping.best_loss:.6f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ab3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predictions = recursive_forecasting(model, test_df, historical_window_size, prediction_window_size, num_of_predictions, torch=True)\n",
    "\n",
    "predictions = [feature_scaler.inverse_transform(prediction) for prediction in predictions]\n",
    "actual = [feature_scaler.inverse_transform(value) for value in actual]\n",
    "\n",
    "lstm_prediction_metrics = evaluate_iaqi_predictions(y_true=actual, y_pred=predictions, prediction_window_size=prediction_window_size, num_of_predictions=num_of_predictions)\n",
    "lstm_prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_metrics_df = create_metrics_dataframe(lstm_prediction_metrics)\n",
    "print(lstm_metrics_df.to_string(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c03247",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ceb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using recursive forecasting - error is cumulative - cannot improve Day 3 prediction without improving Day 1 - so last day's results are enough\n",
    "last_day_metrics = get_day_n_metrics(xgboost_prediction_metrics, prediction_window_size * num_of_predictions)\n",
    "# Using single metric for model comparison - The Willmott index - it gives credit for correlation but heavily penalizes systematic errors that would make the forecasts unreliable for air quality management.\n",
    "metrics = last_day_metrics[\"Willmott\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_model = HopsworksClient().save_model(PROJECT_ROOT, xgboost_model, metrics, X_flat_test[0], y_flat_test[0], feature_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a50423",
   "metadata": {},
   "source": [
    "## Real world prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled for fast iteration (for full test - uncomment)\n",
    "# hopsworks_model, multi_regressor = HopsworksClient().load_model(version=hopsworks_model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to inference module\n",
    "\n",
    "# Take last {historical_window_size} items for {prediction_window_size} predictions \n",
    "X = merged_df[-historical_window_size:]\n",
    "X = feature_scaler.transform(X)\n",
    "\n",
    "for day_index in range(num_of_predictions):\n",
    "    # Input expects multiple windows\n",
    "    X_flat = _flatten_windows([X])\n",
    "\n",
    "    y_pred = xgboost_model.predict(X_flat)\n",
    "\n",
    "    # Split y_pred into 3 arrays, one for each prediction day\n",
    "    y_pred_split = np.split(y_pred.flatten(), prediction_window_size)\n",
    "\n",
    "    # Create DataFrame for predictions, each row is a prediction day\n",
    "    predictions_df = pd.DataFrame(y_pred_split, columns=target_columns)\n",
    "\n",
    "    # Set the index to continue from the last date in merged_df\n",
    "    last_date = X.index[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=prediction_window_size, freq=\"D\")\n",
    "    predictions_df.index = future_dates\n",
    "\n",
    "    # Add predictions to the end so that we can use them as input (don't forget to remove the same number of items as we added - model expects certain size)\n",
    "    X = pd.concat([X[prediction_window_size:], predictions_df], axis=0)\n",
    "\n",
    "X = feature_scaler.inverse_transform(X)\n",
    "\n",
    "# Definition of Air Quality Index is maximum value of Individual Air Quality Indexes\n",
    "X[\"aqi\"] = X[IAQI_FEATURES].max(axis=1)\n",
    "\n",
    "result = X[-num_of_predictions*prediction_window_size:][IAQI_FEATURES]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e35a8c",
   "metadata": {},
   "source": [
    "## Hopsworks deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ae152",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = HopsworksClient().deploy_model(hopsworks_model, overwrite=True)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a77fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for already deployed model\n",
    "\n",
    "# deployment_name = \"aqipredictionmodeldeployment\"\n",
    "# model_serving = HopsworksClient().project.get_model_serving()\n",
    "\n",
    "# deployment = model_serving.get_deployment(deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85e2804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f04cbb0b074485a4f5ed98b411c7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n",
      "2025-08-05 16:06:59,938 WARNING: UserWarning: Failed to patch SSL settings for unverified requests (unsupported version of urllib3?)\n",
      "This may lead to errors when urllib3 tries to modify verify_mode.\n",
      "Please report an issue at https://gitlab.com/alelec/pip-system-certs with your\n",
      "python version included in the description\n",
      "\n",
      "\n",
      "{'predictions': '{\"pm25\":{\"1754438400000\":38.8687515259,\"1754524800000\":39.4622612,\"1754611200000\":44.2506866455},\"pm10\":{\"1754438400000\":14.111992836,\"1754524800000\":15.1734304428,\"1754611200000\":14.6231155396},\"no2\":{\"1754438400000\":2.5830421448,\"1754524800000\":3.107077837,\"1754611200000\":2.6003279686},\"so2\":{\"1754438400000\":2.7117853165,\"1754524800000\":2.931812048,\"1754611200000\":2.7034041882},\"co\":{\"1754438400000\":4.2741427422,\"1754524800000\":3.9961788654,\"1754611200000\":3.9455645084}}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ad659c5cb841df8156404c9a5d5038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deployment.start(await_running=300)\n",
    "# make predictions\n",
    "predictions = deployment.predict({\"instances\":[[\"not_empty\"]]})\n",
    "print(predictions)\n",
    "deployment.stop(await_stopped=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs(component='predictor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
